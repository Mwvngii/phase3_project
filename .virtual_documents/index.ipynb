














# Importing required libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import roc_curve, auc


# Loading data
airline_data = pd.read_csv('Data/airline_passenger_satisfaction.csv')
airline_data.head()


# Dropping Unnecessary columns
airline_data.drop(columns=['Unnamed: 0'], inplace=True)

airline_data


airline_data.info()


# Statistical Summary of numeric columns
airline_data.describe()





# Checking for missing values by percentage
missing = (airline_data.isnull().sum()/len(airline_data))*100
missing





# Filling in for missing values with median
airline_data['arrival_delay_in_minutes'].fillna(airline_data['arrival_delay_in_minutes'].median(), inplace=True)

# Checking for missing values
airline_data.isnull().sum()


# Checking for duplicate rows
airline_data.duplicated().sum()


# Identifying and separating categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns





# Selecting features(x) and target variable(y)
X = airline_data.drop(columns=['satisfaction'])
y = airline_data['satisfaction'].apply(lambda x: 1 if x == "satisfied" else 0)








class_distr = y.value_counts(normalize=True)

# Plot distribution
plt.figure(figsize=(6,5))
sns.countplot(x=y, palette='Set1')
plt.title('Customer Satisfaction Distribution')
plt.show()








# Categorical Features vs Satisfaction
plt.figure(figsize=(15, 10))
for i, feature in enumerate(categorical_cols):
    plt.subplot(2, 2, i + 1)
    sns.countplot(data=airline_data, x=feature, hue='satisfaction', palette='Set2')
    plt.title(f'{feature} vs Satisfaction')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()








# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)





# Preprocessing Pipeline
preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), numerical_cols), 
                                               ('cat', OneHotEncoder(drop='first'), categorical_cols)])





# Logistic Regression(Baseline Model)
lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', LogisticRegression(max_iter=200, random_state=42))])
lr_pipeline


# Decision Tree Model
dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', DecisionTreeClassifier(random_state=42))])
dt_pipeline


# Random Forest Model
rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', RandomForestClassifier(random_state=42))])
rf_pipeline





# Training Logistic Regression Model
lr_pipeline.fit(X_train, y_train)

# Predictions on test set
y_pred_lr = lr_pipeline.predict(X_test)

# Training Decision Tree Model
dt_pipeline.fit(X_train, y_train)

# Predictions on test set
y_pred_dt = dt_pipeline.predict(X_test)

# Training Random Forest Model
rf_pipeline.fit(X_train, y_train)

# Predictions on test set
y_pred_rf = rf_pipeline.predict(X_test)





# Evaluating Model(Logistic Regression)
print("Logistic Regression Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_lr))
print("Classification Report: \n", classification_report(y_test, y_pred_lr))

# Evaluating Model(Decision Tree)
print("Decision Tree Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_dt))
print("Classification Report: \n", classification_report(y_test, y_pred_dt))

# Model Evaluation(Random Forest Model)
print("Random Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report: \n", classification_report(y_test, y_pred_rf))





# Visualizing different model performances using bar plots
models = pd.Series(['Logistic Regression Model', 'Decision Tree Model', 'Random Forest Model'])
accuracies = [accuracy_score(y_test, y_pred_lr), 
              accuracy_score(y_test, y_pred_dt),
              accuracy_score(y_test, y_pred_rf)]
plt.figure(figsize=(10,6))
sns.barplot(x=models, y=accuracies, palette='Set3')
plt.title('Comparison of Different Models by Accuracy')
plt.ylabel('Accuracy')
plt.show()








# Logistic Regression
y_prob_lr = lr_pipeline.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

# Decision Tree
y_prob_dt = dt_pipeline.predict_proba(X_test)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)
roc_auc_dt = auc(fpr_dt, tpr_dt)

# Random Forest
y_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plotting ROC Curves
plt.figure(figsize=(10,6))
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})', color='blue')
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.2f})', color='green')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})', color='red')

plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Dashed diagonal
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression, Decision Tree, and Random Forest')
plt.legend(loc='lower right')
plt.show()








# Get feature names after preprocessing
feature_names_num = numerical_cols
feature_names_cat = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
feature_names = list(feature_names_num) + list(feature_names_cat)

# Extract feature importances from the Random Forest model
feature_importances = rf_pipeline.named_steps['classifier'].feature_importances_

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})

# Sort the features by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)


# Plot the feature importances(top 15)
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))
plt.title('Feature Importance in the Random Forest Model')
plt.show()








# Defining the parameter grid
param_grid = {'classifier__n_estimators': [50, 100, 200, 300],
              'classifier__max_depth': [10, 20, 30,],
              'classifier__min_samples_split': [2, 5, 10],
              'classifier__min_samples_leaf': [1, 2, 4],
              'classifier__bootstrap': [True, False]
             }


# Using RandomizedSearchCV for hyperparemter tuning 
rf_random_search = RandomizedSearchCV(estimator = rf_pipeline, param_distributions = param_grid, n_iter = 50, cv = 3, verbose = 2, random_state = 42, n_jobs = -1)


# Fitting model to train set
rf_random_search.fit(X_train, y_train)


# Getting best parameters and best accuracy score)
print("Best cross validated accuracy:", rf_random_search.best_score_)
print("Best paremeters found:", rf_random_search.best_params_)





best_rf_model = rf_random_search.best_estimator_
y_pred_rf_best = best_rf_model.predict(X_test)

print("Tuned Random Forest Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf_best))
print("Confusion Matrix \n:" , 
      confusion_matrix(y_test, y_pred_rf_best))
print("Classification Report: \n", 
      classification_report(y_test, y_pred_rf_best))












